{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "Train Epoch: 1 [6336/60000 (11%)]\tLoss: 0.607383\n",
      "0.39\n",
      "Train Epoch: 1 [12736/60000 (21%)]\tLoss: 0.427610\n",
      "0.47\n",
      "Train Epoch: 1 [19136/60000 (32%)]\tLoss: 0.113864\n",
      "0.44\n",
      "Train Epoch: 1 [25536/60000 (43%)]\tLoss: 0.435068\n",
      "0.44\n",
      "Train Epoch: 1 [31936/60000 (53%)]\tLoss: 0.260550\n",
      "0.43\n",
      "Train Epoch: 1 [38336/60000 (64%)]\tLoss: 0.283650\n",
      "0.53\n",
      "Train Epoch: 1 [44736/60000 (75%)]\tLoss: 0.173056\n",
      "0.41\n",
      "Train Epoch: 1 [51136/60000 (85%)]\tLoss: 0.136700\n",
      "0.46\n",
      "Train Epoch: 1 [57536/60000 (96%)]\tLoss: 0.077258\n",
      "\n",
      "Test set: Average loss: 0.1709, Accuracy: 9482/10000 (95%)\n",
      "\n",
      "0.43\n",
      "Train Epoch: 2 [6336/60000 (11%)]\tLoss: 0.166975\n",
      "0.51\n",
      "Train Epoch: 2 [12736/60000 (21%)]\tLoss: 0.332341\n",
      "0.37\n",
      "Train Epoch: 2 [19136/60000 (32%)]\tLoss: 0.116006\n",
      "0.53\n",
      "Train Epoch: 2 [25536/60000 (43%)]\tLoss: 0.030795\n",
      "0.46\n",
      "Train Epoch: 2 [31936/60000 (53%)]\tLoss: 0.022938\n",
      "0.39\n",
      "Train Epoch: 2 [38336/60000 (64%)]\tLoss: 0.121522\n",
      "0.51\n",
      "Train Epoch: 2 [44736/60000 (75%)]\tLoss: 0.057653\n",
      "0.42\n",
      "Train Epoch: 2 [51136/60000 (85%)]\tLoss: 0.101950\n",
      "0.51\n",
      "Train Epoch: 2 [57536/60000 (96%)]\tLoss: 0.378711\n",
      "\n",
      "Test set: Average loss: 0.1316, Accuracy: 9596/10000 (96%)\n",
      "\n",
      "0.42\n",
      "Train Epoch: 3 [6336/60000 (11%)]\tLoss: 0.139621\n",
      "0.49\n",
      "Train Epoch: 3 [12736/60000 (21%)]\tLoss: 0.033896\n",
      "0.45\n",
      "Train Epoch: 3 [19136/60000 (32%)]\tLoss: 0.114406\n",
      "0.47\n",
      "Train Epoch: 3 [25536/60000 (43%)]\tLoss: 0.333860\n",
      "0.5\n",
      "Train Epoch: 3 [31936/60000 (53%)]\tLoss: 0.005614\n",
      "0.49\n",
      "Train Epoch: 3 [38336/60000 (64%)]\tLoss: 0.218573\n",
      "0.47\n",
      "Train Epoch: 3 [44736/60000 (75%)]\tLoss: 0.125748\n",
      "0.33\n",
      "Train Epoch: 3 [51136/60000 (85%)]\tLoss: 0.005023\n",
      "0.47\n",
      "Train Epoch: 3 [57536/60000 (96%)]\tLoss: 0.397039\n",
      "\n",
      "Test set: Average loss: 0.1095, Accuracy: 9663/10000 (97%)\n",
      "\n",
      "0.36\n",
      "Train Epoch: 4 [6336/60000 (11%)]\tLoss: 0.081234\n",
      "0.4\n",
      "Train Epoch: 4 [12736/60000 (21%)]\tLoss: 0.209197\n",
      "0.44\n",
      "Train Epoch: 4 [19136/60000 (32%)]\tLoss: 0.313547\n",
      "0.41\n",
      "Train Epoch: 4 [25536/60000 (43%)]\tLoss: 0.042106\n",
      "0.5\n",
      "Train Epoch: 4 [31936/60000 (53%)]\tLoss: 0.030199\n",
      "0.47\n",
      "Train Epoch: 4 [38336/60000 (64%)]\tLoss: 0.020712\n",
      "0.47\n",
      "Train Epoch: 4 [44736/60000 (75%)]\tLoss: 0.028980\n",
      "0.42\n",
      "Train Epoch: 4 [51136/60000 (85%)]\tLoss: 0.057321\n",
      "0.51\n",
      "Train Epoch: 4 [57536/60000 (96%)]\tLoss: 0.026980\n",
      "\n",
      "Test set: Average loss: 0.1005, Accuracy: 9678/10000 (97%)\n",
      "\n",
      "0.44\n",
      "Train Epoch: 5 [6336/60000 (11%)]\tLoss: 0.022282\n",
      "0.39\n",
      "Train Epoch: 5 [12736/60000 (21%)]\tLoss: 0.037819\n",
      "0.38\n",
      "Train Epoch: 5 [19136/60000 (32%)]\tLoss: 0.026928\n",
      "0.5\n",
      "Train Epoch: 5 [25536/60000 (43%)]\tLoss: 0.091869\n",
      "0.4\n",
      "Train Epoch: 5 [31936/60000 (53%)]\tLoss: 0.112292\n",
      "0.51\n",
      "Train Epoch: 5 [38336/60000 (64%)]\tLoss: 0.115028\n",
      "0.51\n",
      "Train Epoch: 5 [44736/60000 (75%)]\tLoss: 0.005588\n",
      "0.43\n",
      "Train Epoch: 5 [51136/60000 (85%)]\tLoss: 0.005454\n",
      "0.41\n",
      "Train Epoch: 5 [57536/60000 (96%)]\tLoss: 0.129534\n",
      "\n",
      "Test set: Average loss: 0.0857, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "0.39\n",
      "Train Epoch: 6 [6336/60000 (11%)]\tLoss: 0.116398\n",
      "0.45\n",
      "Train Epoch: 6 [12736/60000 (21%)]\tLoss: 0.072091\n",
      "0.52\n",
      "Train Epoch: 6 [19136/60000 (32%)]\tLoss: 0.080722\n",
      "0.42\n",
      "Train Epoch: 6 [25536/60000 (43%)]\tLoss: 0.009893\n",
      "0.52\n",
      "Train Epoch: 6 [31936/60000 (53%)]\tLoss: 0.040297\n",
      "0.42\n",
      "Train Epoch: 6 [38336/60000 (64%)]\tLoss: 0.236383\n",
      "0.45\n",
      "Train Epoch: 6 [44736/60000 (75%)]\tLoss: 0.012250\n",
      "0.42\n",
      "Train Epoch: 6 [51136/60000 (85%)]\tLoss: 0.032067\n",
      "0.45\n",
      "Train Epoch: 6 [57536/60000 (96%)]\tLoss: 0.107775\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "0.45\n",
      "Train Epoch: 7 [6336/60000 (11%)]\tLoss: 0.059338\n",
      "0.39\n",
      "Train Epoch: 7 [12736/60000 (21%)]\tLoss: 0.100650\n",
      "0.51\n",
      "Train Epoch: 7 [19136/60000 (32%)]\tLoss: 0.018211\n",
      "0.55\n",
      "Train Epoch: 7 [25536/60000 (43%)]\tLoss: 0.044310\n",
      "0.48\n",
      "Train Epoch: 7 [31936/60000 (53%)]\tLoss: 0.146305\n",
      "0.37\n",
      "Train Epoch: 7 [38336/60000 (64%)]\tLoss: 0.057313\n",
      "0.46\n",
      "Train Epoch: 7 [44736/60000 (75%)]\tLoss: 0.002877\n",
      "0.47\n",
      "Train Epoch: 7 [51136/60000 (85%)]\tLoss: 0.061338\n",
      "0.46\n",
      "Train Epoch: 7 [57536/60000 (96%)]\tLoss: 0.060107\n",
      "\n",
      "Test set: Average loss: 0.0836, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "0.44\n",
      "Train Epoch: 8 [6336/60000 (11%)]\tLoss: 0.121389\n",
      "0.36\n",
      "Train Epoch: 8 [12736/60000 (21%)]\tLoss: 0.041808\n",
      "0.53\n",
      "Train Epoch: 8 [19136/60000 (32%)]\tLoss: 0.053856\n",
      "0.38\n",
      "Train Epoch: 8 [25536/60000 (43%)]\tLoss: 0.049247\n",
      "0.48\n",
      "Train Epoch: 8 [31936/60000 (53%)]\tLoss: 0.011847\n",
      "0.48\n",
      "Train Epoch: 8 [38336/60000 (64%)]\tLoss: 0.009342\n",
      "0.43\n",
      "Train Epoch: 8 [44736/60000 (75%)]\tLoss: 0.021921\n",
      "0.49\n",
      "Train Epoch: 8 [51136/60000 (85%)]\tLoss: 0.115018\n",
      "0.53\n",
      "Train Epoch: 8 [57536/60000 (96%)]\tLoss: 0.024576\n",
      "\n",
      "Test set: Average loss: 0.0934, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "0.46\n",
      "Train Epoch: 9 [6336/60000 (11%)]\tLoss: 0.002837\n",
      "0.53\n",
      "Train Epoch: 9 [12736/60000 (21%)]\tLoss: 0.273209\n",
      "0.36\n",
      "Train Epoch: 9 [19136/60000 (32%)]\tLoss: 0.011819\n",
      "0.48\n",
      "Train Epoch: 9 [25536/60000 (43%)]\tLoss: 0.011853\n",
      "0.41\n",
      "Train Epoch: 9 [31936/60000 (53%)]\tLoss: 0.062831\n",
      "0.58\n",
      "Train Epoch: 9 [38336/60000 (64%)]\tLoss: 0.482841\n",
      "0.5\n",
      "Train Epoch: 9 [44736/60000 (75%)]\tLoss: 0.045928\n",
      "0.5\n",
      "Train Epoch: 9 [51136/60000 (85%)]\tLoss: 0.085416\n",
      "0.47\n",
      "Train Epoch: 9 [57536/60000 (96%)]\tLoss: 0.124807\n",
      "\n",
      "Test set: Average loss: 0.0879, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "0.51\n",
      "Train Epoch: 10 [6336/60000 (11%)]\tLoss: 0.098924\n",
      "0.54\n",
      "Train Epoch: 10 [12736/60000 (21%)]\tLoss: 0.002807\n",
      "0.33\n",
      "Train Epoch: 10 [19136/60000 (32%)]\tLoss: 0.078280\n",
      "0.54\n",
      "Train Epoch: 10 [25536/60000 (43%)]\tLoss: 0.080094\n",
      "0.37\n",
      "Train Epoch: 10 [31936/60000 (53%)]\tLoss: 0.003544\n",
      "0.41\n",
      "Train Epoch: 10 [38336/60000 (64%)]\tLoss: 0.116547\n",
      "0.48\n",
      "Train Epoch: 10 [44736/60000 (75%)]\tLoss: 0.006684\n",
      "0.46\n",
      "Train Epoch: 10 [51136/60000 (85%)]\tLoss: 0.001410\n",
      "0.49\n",
      "Train Epoch: 10 [57536/60000 (96%)]\tLoss: 0.022147\n",
      "\n",
      "Test set: Average loss: 0.0785, Accuracy: 9759/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from liamlib import dotdict\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "RUN_NAME = 'checkgrad'\n",
    "writer = SummaryWriter(log_dir=f'./tboard/fc/{RUN_NAME}')\n",
    "args = dotdict.DotDict()\n",
    "\n",
    "args.seed = 1\n",
    "args.batch_size = 64\n",
    "args.test_batch_size = 100\n",
    "args.lr = 0.02\n",
    "args.momentum = 0.9\n",
    "args.epochs = 10\n",
    "args.log_interval = 100\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            prog = 100. * batch_idx / len(train_loader)\n",
    "            l = loss.item()\n",
    "            batches = (epoch - 1) * len(train_loader.dataset) + batch_idx * len(data)\n",
    "            writer.add_scalar('data/train_loss', l, batches)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), prog, l))\n",
    "\n",
    "def test(args, model, device, test_loader, writer, batches):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), acc))\n",
    "    writer.add_scalar('data/test_loss', test_loss, batches)\n",
    "    writer.add_scalar('data/test_acc', acc, batches)\n",
    "    \n",
    "def train_checkgrad(args, model, device, train_loader, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    accepts = 0\n",
    "    accept_or_not_accept = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        half = len(data) // 2\n",
    "        data0, data1 = data[:half], data[half:]\n",
    "        target0, target1 = target[:half], target[half:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output1 = model(data1)\n",
    "            loss_before = F.nll_loss(output1, target1).item()\n",
    "        \n",
    "        old_weights = deepcopy(model.state_dict())\n",
    "        old_opt_state = deepcopy(optimizer.state_dict())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output0 = model(data0)\n",
    "        loss = F.nll_loss(output0, target0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output1 = model(data1)\n",
    "            loss_after = F.nll_loss(output1, target1).item()\n",
    "        if loss_after >= loss_before:\n",
    "            model.load_state_dict(old_weights)\n",
    "            optimizer.load_state_dict(old_opt_state)\n",
    "        else:\n",
    "            accepts += 1\n",
    "        accept_or_not_accept += 1\n",
    "\n",
    "        if batch_idx % args.log_interval == (args.log_interval - 1):\n",
    "            prog = 100. * batch_idx / len(train_loader)\n",
    "            l = loss.item()\n",
    "            aa = accepts / accept_or_not_accept\n",
    "            batches = (epoch - 1) * len(train_loader.dataset) + batch_idx * len(data)\n",
    "            writer.add_scalar('data/train_loss', l, batches)\n",
    "            writer.add_scalar('data/accept_rate', aa, batches)\n",
    "            print(aa)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), prog, l))\n",
    "            accepts = 0\n",
    "            accept_or_not_accept = 0\n",
    "\n",
    "if 'checkgrad' in RUN_NAME:\n",
    "    train = train_checkgrad\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch, writer)\n",
    "    test(args, model, device, test_loader, writer, epoch * len(train_loader))\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36'\n'",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
